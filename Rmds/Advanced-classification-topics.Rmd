---
title: "Advanced Topics in Classification"
subtitle: "Multinomial Logistic Regression"
output: pdf_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "..") }) 
date: "07-14-2023"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(nnet)
library(xgboost)
library(plotROC)
library(ranger)
library(SHAPforxgboost)
```

# Example NFL Points

Driving Possibilities

* No Score

* Field Goal

* Touchdown

* Safety

Next scoring play:

* For: touchdown, field goal, safety

* Against: -touchdown, -field goal, -safety

* No score

(treating point-after-touchdown attempts separtely)

**Expected points:** measure the value of the play in terms of the expected points in the next scoring play

We want to **estimate the probabilities** of each scoring event to compute expected points

*How do we model more than two categories?*

We can extend this to K classes (via the **softmax function**)

* We only estimate coefficients for the K-1 classes **relative to reference class**

$$P(Y = k^* | X = x) = \frac{e^{\beta_{0k}+...\beta_{pk}X_p}}{\sum_{k-1}^{K}e^{\beta_{0k}+...\beta_{pk}X_p}}$$

In the expected score for NFL, the model is specified with **six logit transfromations** relative to **No Score**

```{r}
nfl_ep_model_data <- readRDS(url("https://shorturl.at/BTVZ1"))

nfl_ep_model_data <- nfl_ep_model_data %>%
  mutate(Next_Score_Half = fct_relevel(Next_Score_Half, "No_Score"), 
         # log transform of yards to go and indicator for two minute warning:
         log_ydstogo = log(ydstogo), 
         # Changing down into a factor variable:
         down = factor(down))
```

```{r}
init_ep_model <- multinom(Next_Score_Half ~ half_seconds_remaining + yardline_100 + down +                            log_ydstogo + log_ydstogo*down + yardline_100*down, 
                          data = nfl_ep_model_data, maxit = 300)
```

## Leave-one-season-out cross-validation

```{r eval = FALSE}
init_loso_cv_preds <- map_dfr(unique(nfl_ep_model_data$season), function(x) 
  { # Separate test and training data:
  test_data <- nfl_ep_model_data %>% filter(season == x)
  train_data <- nfl_ep_model_data %>% filter(season != x) 
  # Fit multinomial logistic regression model:
  ep_model <-  multinom(Next_Score_Half ~ half_seconds_remaining + yardline_100 + down +                          log_ydstogo + log_ydstogo*down + yardline_100*down,  data = train_data, maxit = 300) 
  # Return dataset of class probabilities:
  predict(ep_model, newdata = test_data, type = "probs") %>%  as_tibble() %>% 
    mutate(Next_Score_Half = test_data$Next_Score_Half,season = x)})
```

## Calibration Results

```{r eval = FALSE}
ep_cv_loso_calibration_results <- init_loso_cv_preds %>%
  pivot_longer(No_Score:Touchdown,
               names_to = "next_score_type", values_to = "pred_prob") %>% 
  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) %>%
  group_by(next_score_type, bin_pred_prob) %>%
  summarize(n_plays = n(), n_scoring_event = length(which(Next_Score_Half == next_score_type)),
            bin_actual_prob = n_scoring_event / n_plays,
            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays)) %>%
  ungroup() %>%  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),
                        bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0))
```

## Calibration Results for each Scoring Event

```{r eval = FALSE}
ep_cv_loso_calibration_results %>%
  mutate(next_score_type = fct_relevel(next_score_type, "Opp_Safety", "Opp_Field_Goal", "Opp_Touchdown", "No_Score", "Safety", "Field_Goal", "Touchdown"),
         next_score_type = fct_recode(next_score_type, "-Field Goal (-3)" = "Opp_Field_Goal", "-Safety (-2)" = "Opp_Safety", "-Touchdown (-7)" = "Opp_Touchdown", "Field Goal (3)" = "Field_Goal", "No Score (0)" = "No_Score", "Touchdown (7)" = "Touchdown", "Safety (2)" = "Safety")) %>%
  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +  geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
  geom_smooth(se = FALSE) +
  geom_point(aes(size = n_plays)) +
  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper))+
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1)) +   labs(size = "Number of plays", x = "Estimated next score probability", y = "Observed next score probability") +
  theme_bw() +
  theme(strip.background = element_blank(),
        axis.text.x = element_text(angle = 90), 
        legend.position = c(1, .05),
        legend.justification = c(1, 0)) +
  facet_wrap(~ next_score_type, ncol = 4)
```

## Multinomial Classification with XGBoost

```{r eval = FALSE}
nfl_ep_model_data <- nfl_ep_model_data %>%
  mutate(Next_Score_Half = fct_relevel(Next_Score_Half, "No_Score", "Safety", "Field_Goal", "Touchdown", "Opp_Safety", "Opp_Field_Goal", "Opp_Touchdown"),
         next_score_label = as.numeric(Next_Score_Half) - 1)

model_variables <- c("half_seconds_remaining", "yardline_100", "down", "ydstogo")
```

```{r eval = FALSE}
## calibration plots!

xgb_loso_cv_preds <-   map_dfr(unique(nfl_ep_model_data$season), function(x) { 
  # Separate test and training data - scale variables:
  test_data <- nfl_ep_model_data %>% filter(season == x)
  test_data_x <- as.matrix(dplyr::select(test_data, model_variables))
  train_data <- nfl_ep_model_data %>% filter(season != x)            
  train_data_x <- as.matrix(dplyr::select(train_data, model_variables))
  train_data_y <- train_data$next_score_label
  xgb_model <- xgboost(data = train_data_x, label = train_data_y, nrounds = 100,                                 max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 1,                                 min_child_weight = 1, subsample = 1, nthread = 1, 
                       objective = 'multi:softprob', num_class = 7,
                       eval_metric = 'mlogloss', verbose = 0)
xgb_preds <- matrix(predict(xgb_model, test_data_x), ncol = 7, byrow = TRUE) %>%
  as_tibble()

colnames(xgb_preds) <- c("No_Score", "Safety", "Field_Goal", "Touchdown", "Opp_Safety", "Opp_Field_Goal", "Opp_Touchdown")

xgb_preds %>%
  mutate(Next_Score_Half = test_data$Next_Score_Half,season = x)
})
```

```{r eval = FALSE}
ep_cv_loso_calibration_results <- xgb_loso_cv_preds %>%
  pivot_longer(No_Score:Opp_Touchdown,
               names_to = "next_score_type", values_to = "pred_prob") %>%
  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) %>%
  group_by(next_score_type, bin_pred_prob) %>%
  summarize(n_plays = n(), 
            n_scoring_event = length(which(Next_Score_Half == next_score_type)),
            bin_actual_prob = n_scoring_event / n_plays,
            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays)) %>%
  ungroup() %>%
  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),
         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0))
```


```{r eval = FALSE}
ep_cv_loso_calibration_results %>%
  mutate(next_score_type = fct_relevel(next_score_type, "Opp_Safety", "Opp_Field_Goal", "Opp_Touchdown", "No_Score", "Safety", "Field_Goal", "Touchdown"),
         next_score_type = fct_recode(next_score_type, "-Field Goal (-3)" = "Opp_Field_Goal", "-Safety (-2)" = "Opp_Safety", "-Touchdown (-7)" = "Opp_Touchdown", "Field Goal (3)" = "Field_Goal", "No Score (0)" = "No_Score", "Touchdown (7)" = "Touchdown", "Safety (2)" = "Safety")) %>%
  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +
  geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
  geom_smooth(se = FALSE) +
  geom_point(aes(size = n_plays)) +
  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper))+
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(size = "Number of plays", x = "Estimated next score probability",
       y = "Observed next score probability") +
  theme_bw() +
  theme(strip.background = element_blank(),
        axis.text.x = element_text(angle = 90),
        legend.position = c(1, .05),
        legend.justification = c(1, 0)) +
  facet_wrap(~ next_score_type, ncol = 4)
```


# Model Evaluation for Classification


```{r}
nfl_passing_plays <-
  read_csv("https://shorturl.at/ADMWZ") %>% 
  # Only keep rows with passer and receiver information known:
  filter(!is.na(passer_player_id), !is.na(receiver_player_id),
         !is.na(epa), !is.na(air_yards), !is.na(pass_location)) %>% 
  # Combine passer and receiver unique IDs:
  mutate(passer_name_id = paste0(passer_player_name, ":", passer_player_id),
         receiver_name_id = paste0(receiver_player_name, ":", receiver_player_id))
```

```{r}
set.seed(1985)

game_fold_table <- tibble(game_id = unique(nfl_passing_plays$game_id)) %>%
  mutate(game_fold = sample(rep(1:5, length.out = n()), n()))
nfl_passing_plays <- nfl_passing_plays %>%
  dplyr::left_join(game_fold_table, by = "game_id")
```

```{r}
logit_cv_preds <- map_dfr(unique(nfl_passing_plays$game_fold),
                          function(test_fold) { 
# Separate test and training data:
                            test_data <- nfl_passing_plays %>%
                              filter(game_fold == test_fold)
                            train_data <- nfl_passing_plays %>%
                              filter(game_fold != test_fold) 
# Train model:
                            logit_model <- glm(complete_pass ~ yardline_100 + shotgun + air_yards +                                  pass_location + qb_hit, data = train_data,family = "binomial") 
# Return tibble of holdout results:
                            tibble(test_pred_probs = predict(logit_model, newdata = test_data,                                             type = "response"), test_actual = test_data$complete_pass,
                                   game_fold = test_fold)
                            })
```

```{r}
logit_cv_preds %>%
  mutate(test_pred = ifelse(test_pred_probs < .5, 0, 1)) %>%
  group_by(game_fold) %>%
  summarize(mcr = mean(test_pred != test_actual))
```

## Evaluating the prediction threshold

* Accuracy

* Precision

* Sensitivity

* Specificity 

**Generally, we want to balance high power and low false rate**

### Receiver Operating Characteristic (ROC) Curve

**We want to maximize the area under the curve**

```{r}
logit_cv_preds %>%
  ggplot() +
  geom_roc(aes(d = test_actual,
               m = test_pred_probs), labelround = 4) +
  style_roc() +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray") +
  labs(color = "Test fold")

with(logit_cv_preds, MLmetrics::AUC(test_pred_probs, test_actual))
```


```{r}
logit_cv_preds %>%  ggplot() +   geom_roc(aes(d = test_actual,
                                              m = test_pred_probs,
                                              color = as.factor(game_fold)),
                                          n.cuts = 0) +
  style_roc() +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray") +
  ggthemes::scale_color_colorblind() +
  labs(color = "Test fold") +
  theme(legend.position = "bottom") 

logit_cv_preds %>% group_by(game_fold) %>%
  summarize(auc = MLmetrics::AUC(test_pred_probs, test_actual))
```

# Tree-based Approach to Logistic Regression

```{r}
# converting categorical variables into dummy indicators
model_data <- nfl_passing_plays %>%
  mutate(play_id = 1:n(),
         complete_pass = as.factor(complete_pass)) %>%
  dplyr::select(play_id, complete_pass, yardline_100, shotgun, air_yards, qb_hit,
                game_fold, pass_location) %>%
  mutate(pass_location_val = 1) %>%
  pivot_wider(id_cols = play_id:game_fold, 
              names_from = pass_location, values_from = pass_location_val,
              values_fill = 0) %>%
  dplyr::select(-play_id)
```


```{r}
# for each tree, compute class proportion in terminal node, then take average across all trees

rf_prob_cv_preds <-   map_dfr(unique(model_data$game_fold),
                              function(test_fold) { 
# Separate test and training data - scale variables:
                                test_data <- model_data %>%
                                  filter(game_fold == test_fold)
                                
                                train_data <- model_data %>%
                                  filter(game_fold != test_fold)
                                rf_prob_model <- ranger(complete_pass ~ .,
                                  data = dplyr::select(train_data, -game_fold), 
                                  probability = TRUE)
# Return tibble of holdout results:
                                tibble(test_pred_probs = 
                                         as.numeric(predict(rf_prob_model, data = test_data,
                                                            type = "response")$predictions[,2]),
                                       test_actual = as.numeric(test_data$complete_pass) - 1,
                                       game_fold = test_fold)
                                })
```


```{r}
rf_prob_cv_preds %>%
  ggplot() +
  geom_roc(aes(d = test_actual, m = test_pred_probs, color = as.factor(game_fold)),
           n.cuts = 0) +
  style_roc() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  ggthemes::scale_color_colorblind() +
  labs(color = "Test fold") +
  theme(legend.position = "bottom") 

rf_prob_cv_preds %>%
  group_by(game_fold) %>%
  summarize(auc = MLmetrics::AUC(test_pred_probs, test_actual))
```

```{r}
xgb_cv_preds <- 
  map_dfr(unique(model_data$game_fold),
          function(test_fold) { 
# Separate test and training data - scale variables:
            test_data <- model_data %>% filter(game_fold == test_fold)
            test_data_x <- as.matrix(dplyr::select(test_data, -complete_pass, -game_fold))
            
            train_data <- model_data %>%
              filter(game_fold != test_fold)
            train_data_x <- as.matrix(dplyr::select(train_data, -complete_pass, -game_fold))
            train_data_y <- as.numeric(train_data$complete_pass) - 1
            
            xgb_model <- xgboost(data = train_data_x, label = train_data_y,
                                 nrounds = 100, max_depth = 3, eta = 0.3,
                                 gamma = 0, colsample_bytree = 1, min_child_weight = 1,
                                 subsample = 1, nthread = 1,
                                 objective = 'binary:logistic', eval_metric = 'auc',                                  verbose = 0) 
# Return tibble of holdout results:
            tibble(test_pred_probs = as.numeric(predict(xgb_model, newdata = test_data_x, 
                                                        type = "response")),
                   test_actual = as.numeric(test_data$complete_pass) - 1,
                   game_fold = test_fold) 
          })
```

```{r}
xgb_cv_preds %>%
  ggplot() +
  geom_roc(aes(d = test_actual,
               m = test_pred_probs, color = as.factor(game_fold)), n.cuts = 0) +
  style_roc() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  ggthemes::scale_color_colorblind() +
  labs(color = "Test fold") +
  theme(legend.position = "bottom") 

xgb_cv_preds %>%
  group_by(game_fold) %>%
  summarize(auc = MLmetrics::AUC(test_pred_probs, test_actual))
```

```{r}
bind_rows(  mutate(logit_cv_preds, type = "logit"),
            mutate(rf_prob_cv_preds, type = "RF"),
            mutate(xgb_cv_preds, type = "XGBoost")) %>%
  ggplot() +
  geom_roc(aes(d = test_actual, m = test_pred_probs, color = type),
           n.cuts = 0) +
  style_roc() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  ggthemes::scale_color_colorblind() +
  labs(color = "Model") +
  theme(legend.position = "bottom")

# pretty similar performance across all models...
```

# Explaining Predictoins with SHAP-values

SHAP-values are based on *Shapley values* (an idea from game theory) and are used to measure the contribution from each feature in the model to the prediction for an individual observation

* The Shapley value for feature *j* for observation *i* can be interpreted as:

The value of feature *j* contribution to the prediction of observation *i* compared to the average prediction for the data set

Linear regression coefficients function in the same way

```{r}
train_data_x <- as.matrix(dplyr::select(model_data, -complete_pass, -game_fold))
train_data_y <- as.numeric(model_data$complete_pass) - 1

xgb_model <- xgboost(data = train_data_x, label = train_data_y, nrounds = 100, max_depth = 3,   
                     eta = 0.3, gamma = 0, colsample_bytree = 1, min_child_weight = 1,
                     subsample = 1, nthread = 1, objective = 'binary:logistic',
                     eval_metric = 'auc', verbose = 0)

shap_value_list <- shap.values(xgb_model, X_train = train_data_x)
shap.plot.summary.wrap1(xgb_model, X = train_data_x)
```

